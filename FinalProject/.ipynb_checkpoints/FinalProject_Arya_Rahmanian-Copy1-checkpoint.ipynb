{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a116e89e-6588-424a-b975-8690900716e8",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## CSCE 633\n",
    "## Arya Rahmanian\n",
    "## Summer 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6918d-63c6-4660-96b9-3b9aab2dfb6e",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e07bff3-7c39-4dde-aa45-654cfd79f7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text  stars\n",
      "117921  This was a tough one.  After having heard good...    3.0\n",
      "117777  I have been here probably 3 times in the last ...    3.0\n",
      "172811  I don't usually do chain restaurants, but I st...    4.0\n",
      "63245   This place is just out there... There main onl...    1.0\n",
      "87607   Went here for Happy Hour one evening. Twas a g...    4.0\n",
      "                                                    text  stars\n",
      "13742  Sadly I tried chipotle again after the last re...    3.0\n",
      "12304  This place sells really good fresh corn tortil...    4.0\n",
      "6107   Having only recently moved to PA, I'm still ge...    4.0\n",
      "2796   This is by far my favorite breakfast spot in N...    5.0\n",
      "3379   This is a great store. I come here often, ever...    5.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load data\n",
    "train_df = pd.read_csv(\"yelp_review_train.csv\")\n",
    "test_df = pd.read_csv(\"yelp_review_test.csv\")\n",
    "\n",
    "#randomly select data points for training, validation, and testing\n",
    "train_data = train_df.sample(n=10000, random_state=42)\n",
    "val_data = train_df.drop(train_data.index).sample(n=1000, random_state=42)\n",
    "test_data = test_df.sample(n=2000, random_state=42)\n",
    "\n",
    "print(train_data.head())\n",
    "#print(val_data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d40a136-0129-44c9-bc6e-38c06e86d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    embedding_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embedding_dict[word] = vector\n",
    "    return embedding_dict\n",
    "\n",
    "\n",
    "glove_file = 'glove.twitter.27B.200d.txt'  \n",
    "\n",
    "embedding_dict = load_glove_embeddings(glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f502c828-6b04-4f06-bcf4-dca94bd72064",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94fa5e05-19ec-4351-9dea-964b0c7b638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(stars):\n",
    "    return 0 if stars < 4 else 1\n",
    "\n",
    "# create our label column\n",
    "train_data['label'] = train_data['stars'].apply(map_labels)\n",
    "test_data['label'] = test_data['stars'].apply(map_labels)\n",
    "val_data['label'] = val_data['stars'].apply(map_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb38ce9b-96ad-4420-ba9a-912fe2b8933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Airsight\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(preprocess_text)\n",
    "val_data['text'] = val_data['text'].apply(preprocess_text)\n",
    "test_data['text'] = test_data['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057554c-7151-4d64-b2f8-552422456cc2",
   "metadata": {},
   "source": [
    "### Tokenize and Pad Text Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c716a79-71d4-43e4-8952-d9a417acbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 300)\n",
      "(1000, 300)\n",
      "(2000, 300)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['text'])\n",
    "val_sequences = tokenizer.texts_to_sequences(val_data['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "# max length for sequences\n",
    "max_length = 300\n",
    "\n",
    "pad_last_words = True\n",
    "if(pad_last_words):\n",
    "    # Pad the last n characters\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='pre')\n",
    "    val_padded = pad_sequences(val_sequences, maxlen=max_length, padding='post', truncating='pre')\n",
    "    test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='pre')\n",
    "else:\n",
    "    # Pad the first n characters\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    val_padded = pad_sequences(val_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# shape of padded sequences\n",
    "print(train_padded.shape)\n",
    "print(val_padded.shape)\n",
    "print(test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4776748d-1cb6-487a-9f06-3c2cd3557f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Create embedding matrix\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 200  # GloVe dimension\n",
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index >= vocab_size:\n",
    "        continue\n",
    "    embedding_vector = embedding_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# Convert embedding matrix to tensor\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2e052-e3b8-4f68-8fd5-162cc79b81bc",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5e984-c66d-4c35-a4e3-2ce86d57fe85",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6e63601-75be-425f-b4fa-2df85cd14d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerSentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, num_classes, max_length, embedding_matrix):\n",
    "        super(TransformerSentimentClassifier, self).__init__()\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        \n",
    "        # positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_length, embed_dim))\n",
    "        \n",
    "        # transformer encoder layers\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, max_length, embed_dim)\n",
    "        x += self.positional_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        x = x.permute(1, 0, 2) \n",
    "        x = self.transformer_encoder(x)  # (max_length, batch_size, embed_dim)\n",
    "        \n",
    "        x = x.mean(dim=0)  # (batch_size, embed_dim)\n",
    "        x = self.fc(x)  # (batch_size, num_classes)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b99d6-d0b5-4ccd-b423-cbfd3f495425",
   "metadata": {},
   "source": [
    "#### Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "008b5a21-e1cf-417b-aace-a7ae185c06d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerSentimentClassifier(\n",
      "  (embedding): Embedding(30165, 200)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=200, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=200, bias=True)\n",
      "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Airsight\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "embed_dim = 200\n",
    "num_heads = 8\n",
    "hidden_dim = 512\n",
    "num_layers = 4\n",
    "num_classes = 2 \n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerSentimentClassifier(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, num_classes, max_length, embedding_matrix)\n",
    "\n",
    "# Display the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771c2ad-8dd6-42c1-a263-ce8cec6c665a",
   "metadata": {},
   "source": [
    "#### Build Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fafb413e-8f91-435c-90ed-c8abb2d8bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# convert to Torch tensors\n",
    "X_train = torch.tensor(train_padded, dtype=torch.long)\n",
    "y_train = torch.tensor(train_data['label'].values, dtype=torch.long)\n",
    "X_val = torch.tensor(val_padded, dtype=torch.long)\n",
    "y_val = torch.tensor(val_data['label'].values, dtype=torch.long)\n",
    "X_test = torch.tensor(test_padded, dtype=torch.long)\n",
    "y_test = torch.tensor(test_data['label'].values, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0f83f-7802-4507-8606-bb4f8cf0a876",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3aecbc3f-095d-473c-ab84-d24ade9d3e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerSentimentClassifier(\n",
       "  (embedding): Embedding(30165, 200)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=200, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW, Adam, SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# lists to store training metrics\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# constants hyper params\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# init optimizer, and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93226e-e9c2-463e-ac33-0f39aebdcf2e",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc5910-f18f-458a-b927-fd684ea313b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad4e58-6104-4326-b29b-cef2e2eec0ba",
   "metadata": {},
   "source": [
    "### Plot Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0f00d-0e01-45b1-9b0d-723b40727416",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, EPOCHS + 1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'r', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cde94b-a15c-428a-8c0c-00a3c6fce256",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7cacb9a1-46be-4855-883b-66cc94d2dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'yelp_classifer_model.pth'\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b2303c-9fbd-4197-8f87-1db0e68034e2",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dda985-2f7c-4de1-ba73-bfe2e8cdd0cc",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "abfabea3-d413-4eab-b42c-34da9d93d418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Airsight\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerSentimentClassifier(\n",
       "  (embedding): Embedding(30165, 200)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=200, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = TransformerSentimentClassifier(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, num_classes, max_length, embedding_matrix)\n",
    "\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3dd04-d4eb-404a-b899-b2feb7f020b4",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e58eb5a-2e1f-44db-8543-152952cfc333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# evaluate model\n",
    "model.eval()\n",
    "\n",
    "# track loss and accuracy\n",
    "test_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# lists to store true and predicted labels for metric calculation\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "# test\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        # store true and predicted labels\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# calc average test loss and accuracy\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "# calculate additional metrics\n",
    "precision = precision_score(all_labels, all_preds, average='binary')\n",
    "recall = recall_score(all_labels, all_preds, average='binary')\n",
    "f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d0361-fd08-4f3e-bca2-6841c2c9bb01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
